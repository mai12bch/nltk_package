{
 "metadata": {
  "name": "",
  "signature": "sha256:be44730ae90b66423a15a633f42442074efcf257603ecfb1ef523581ee679065"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Learning the basics of Natural Language Toolkit (NLTK)\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Installing NLTK "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "This notebook has a few dependencies that can be installed via the python package manager -pip \n",
      "    \n",
      "    1. Python 3.0 or higher \n",
      "    2. NLTK \n",
      "    3. NLTK corpora\n",
      "\n",
      "If Python and pip are installed, you can install NLTK as follows:\n",
      "\n",
      "    ~$ pip install nltk \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Getting started. Before starting is good to install the nltk.data which contains many corpora, toy grammars, trained models, etc. \n",
      "Installation: at http://nltk.org/nltk_data/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This will open up a window with which you can download various corpora and models to a specified location. Take note of the download directory."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preprocessing Text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK is a good tool at the processing of a Raw text - it provides some tools for deviding text into it's constituent parts: \n",
      "\n",
      "- sent_tokenize: a punkt sentence tokenizer \n",
      "    \n",
      "    The sent_tokenize devides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations and words that start sentences. (Must be trained on a large collection of plaintext in the target language before it can be used) \n",
      "    \n",
      "    Punkt is designed to learn parameters (list of abbreviations, etc) unsupervised from a corpus similiar to the target domain. The pre-packaged models may therefore be unsuitable: -> PunktSentenceTokenizer(text) to learn parameters from the given text. \n",
      "    \n",
      "    \n",
      "- word_tokenize: a treebank tokenizer \n",
      "\n",
      "    The Treebank tokenizer that uses regular expressions to tokenize text as in PennTreebank. It assumes that the text has already been segmented into sentences, e.g. using first sent_tokenize() \n",
      "    \n",
      "    \n",
      "- pos_tag: a maximum entropy tagger trained on the Penn Treebank "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sentence tokenization \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is tokenization ? \n",
      "\n",
      "- Tokenization is the process of breaking of a text up into sentences, words, phrases or other meaningful elements called tokens \n",
      "\n",
      "- The tokanizaion is the pre-processing of the text analysis "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start with a short example of tokenizing a text in sentences. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = \"It was a great day - said John. But it was almost going to rain\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we want to split the text in sentences. First we need to import the sentence tokenizing function and after that we can put the text as an argument "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import sent_tokenize "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenize(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "['It was a great day - said John.', 'But it was almost going to rain']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(sent_tokenize(text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What about greek ? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "greek_text = \"\u03a0\u03c1\u03bf\u03ca\u03c3\u03c4\u03bf\u03c1\u03b9\u03ba\u03bf\u03af \u03c0\u03bf\u03bb\u03b9\u03c4\u03b9\u03c3\u03bc\u03bf\u03af \u03ac\u03c1\u03c7\u03b9\u03c3\u03b1\u03bd \u03bd\u03b1 \u03b1\u03bd\u03b1\u03c0\u03c4\u03cd\u03c3\u03c3\u03bf\u03bd\u03c4\u03b1\u03b9 \u03c3\u03c4\u03b1 \u0392\u03bf\u03c5\u03bb\u03b3\u03b1\u03c1\u03b9\u03ba\u03ac \u03b5\u03b4\u03ac\u03c6\u03b7 \u03ba\u03b1\u03c4\u03ac \u03c4\u03b7 \u039d\u03b5\u03bf\u03bb\u03b9\u03b8\u03b9\u03ba\u03ae \u03c0\u03b5\u03c1\u03af\u03bf\u03b4\u03bf. \\\n",
      "\u0397 \u03b1\u03c1\u03c7\u03b1\u03af\u03b1 \u03b9\u03c3\u03c4\u03bf\u03c1\u03af\u03b1 \u03c4\u03b7\u03c2 \u03b3\u03bd\u03ce\u03c1\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u03c0\u03b1\u03c1\u03bf\u03c5\u03c3\u03af\u03b1 \u03c4\u03c9\u03bd \u0398\u03c1\u03b1\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9 \u03b1\u03c1\u03b3\u03cc\u03c4\u03b5\u03c1\u03b1 \u03c4\u03c9\u03bd \u0395\u03bb\u03bb\u03ae\u03bd\u03c9\u03bd \u03ba\u03b1\u03b9 \u03c4\u03c9\u03bd \u03a1\u03c9\u03bc\u03b1\u03af\u03c9\u03bd.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = sent_tokenize(greek_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(k[0])\n",
      "print(k[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u03a0\u03c1\u03bf\u03ca\u03c3\u03c4\u03bf\u03c1\u03b9\u03ba\u03bf\u03af \u03c0\u03bf\u03bb\u03b9\u03c4\u03b9\u03c3\u03bc\u03bf\u03af \u03ac\u03c1\u03c7\u03b9\u03c3\u03b1\u03bd \u03bd\u03b1 \u03b1\u03bd\u03b1\u03c0\u03c4\u03cd\u03c3\u03c3\u03bf\u03bd\u03c4\u03b1\u03b9 \u03c3\u03c4\u03b1 \u0392\u03bf\u03c5\u03bb\u03b3\u03b1\u03c1\u03b9\u03ba\u03ac \u03b5\u03b4\u03ac\u03c6\u03b7 \u03ba\u03b1\u03c4\u03ac \u03c4\u03b7 \u039d\u03b5\u03bf\u03bb\u03b9\u03b8\u03b9\u03ba\u03ae \u03c0\u03b5\u03c1\u03af\u03bf\u03b4\u03bf.\n",
        "\u0397 \u03b1\u03c1\u03c7\u03b1\u03af\u03b1 \u03b9\u03c3\u03c4\u03bf\u03c1\u03af\u03b1 \u03c4\u03b7\u03c2 \u03b3\u03bd\u03ce\u03c1\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u03c0\u03b1\u03c1\u03bf\u03c5\u03c3\u03af\u03b1 \u03c4\u03c9\u03bd \u0398\u03c1\u03b1\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9 \u03b1\u03c1\u03b3\u03cc\u03c4\u03b5\u03c1\u03b1 \u03c4\u03c9\u03bd \u0395\u03bb\u03bb\u03ae\u03bd\u03c9\u03bd \u03ba\u03b1\u03b9 \u03c4\u03c9\u03bd \u03a1\u03c9\u03bc\u03b1\u03af\u03c9\u03bd.\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(k)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The sent_tokinze is working with punctianiational signs. It is good for the most of the european languages. But let's try with spanish "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spanish_text = \"Existen cuatro maneras de responder preguntas. \u00bfCu\u00e1les son esas cuatro maneras?\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenize(spanish_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "['Existen cuatro maneras de responder preguntas.',\n",
        " '\u00bfCu\u00e1les son esas cuatro maneras?']"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "In the nltk.data folder you can find under /tokenizer/punkt a various number of language tokenizer e.g. spanish.pickle "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spanish_tokenizer = nltk.data.load(\"tokenizers/punkt/spanish.pickle\")\n",
      "spanish_tokenizer.tokenize(spanish_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "['Existen cuatro maneras de responder preguntas.',\n",
        " '\u00bfCu\u00e1les son esas cuatro maneras?']"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tokenizing sentences in words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_tokenize('Digital Hummanities')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "['Digital', 'Hummanities']"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = word_tokenize('Digital Hummanities') \n",
      "k[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "'Hummanities'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "It works by separating words using spaces and punctuation. The function does not discard the punctuation. So after the word segmentation, we have to deal with the punctuation it. "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Another try"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_tokenize(\"can't go home\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "['ca', \"n't\", 'go', 'home']"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In that example \"can't\" is separated, as \"ca\" and \"n't\".\n",
      "Alternative word tokenizers are the WordPunktTokenizer and PunktWordTokenizer. "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "WordPunktTokenizer splits all the punctuation into separate tokens"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import WordPunctTokenizer \n",
      "word_punct_tokenizer = WordPunctTokenizer()\n",
      "word_punct_tokenizer.tokenize(\"Can't jump today\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "['Can', \"'\", 't', 'jump', 'today']"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "PunktTokenizer splits on punctuation but keeps it within the word "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import PunktWordTokenizer \n",
      "punkt_word_tokenizer = PunktWordTokenizer()\n",
      "punkt_word_tokenizer.tokenize(\"Can't jump today\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "['Can', \"'t\", 'jump', 'today']"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Import the german.txt in the texts.file and try to tokenize the sentences and the words using te german.pickle  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Part-of-speech tagging "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Part-of-speech tagging is the process of converting a sentence in a list of words, grouping them into tuples (word,tag). \n",
      "The tag is a part-of-speech tag, which tells if the word is a noun, adjective, verb and so on. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = word_tokenize(\"It was a sunny and warm day, so we went to the beach\")\n",
      "nltk.pos_tag(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "[('It', 'PRP'),\n",
        " ('was', 'VBD'),\n",
        " ('a', 'DT'),\n",
        " ('sunny', 'JJ'),\n",
        " ('and', 'CC'),\n",
        " ('warm', 'NN'),\n",
        " ('day', 'NN'),\n",
        " (',', ','),\n",
        " ('so', 'RB'),\n",
        " ('we', 'PRP'),\n",
        " ('went', 'VBD'),\n",
        " ('to', 'TO'),\n",
        " ('the', 'DT'),\n",
        " ('beach', 'NN')]"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The pos_tag uses the PennTreebank."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = word_tokenize(\"Monty Python (sometimes known as The Pythons) were a British surreal comedy group who created their sketch comedy show Monty Python's Flying Circus,  th at first aired on the BBC on 5 October 1969.\")\n",
      "\n",
      "nltk.pos_tag(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "[('Monty', 'NN'),\n",
        " ('Python', 'NNP'),\n",
        " ('(', 'NNP'),\n",
        " ('sometimes', 'RB'),\n",
        " ('known', 'VBN'),\n",
        " ('as', 'IN'),\n",
        " ('The', 'DT'),\n",
        " ('Pythons', 'NNS'),\n",
        " (')', ':'),\n",
        " ('were', 'VBD'),\n",
        " ('a', 'DT'),\n",
        " ('British', 'JJ'),\n",
        " ('surreal', 'NN'),\n",
        " ('comedy', 'NN'),\n",
        " ('group', 'NN'),\n",
        " ('who', 'WP'),\n",
        " ('created', 'VBN'),\n",
        " ('their', 'PRP$'),\n",
        " ('sketch', 'NN'),\n",
        " ('comedy', 'NN'),\n",
        " ('show', 'NN'),\n",
        " ('Monty', 'NNP'),\n",
        " ('Python', 'NNP'),\n",
        " (\"'s\", 'POS'),\n",
        " ('Flying', 'NNP'),\n",
        " ('Circus', 'NNP'),\n",
        " (',', ','),\n",
        " ('th', 'NN'),\n",
        " ('at', 'IN'),\n",
        " ('first', 'JJ'),\n",
        " ('aired', 'VBN'),\n",
        " ('on', 'IN'),\n",
        " ('the', 'DT'),\n",
        " ('BBC', 'NNP'),\n",
        " ('on', 'IN'),\n",
        " ('5', 'CD'),\n",
        " ('October', 'NNP'),\n",
        " ('1969', 'CD'),\n",
        " ('.', '.')]"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Another interesting function is the text.similar function. It takes a word \"w\" and find in a given corpus all the pairs \"w1\"w\"\"w2\" \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
      " text.similar('greek')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "the history time man af war english water spirit best government\n",
        "congress weather them modern public right america other society\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Word frequencies "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Filtering stop words"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5. Cooccurrences (and ngrams)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}