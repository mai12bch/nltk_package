{
 "metadata": {
  "name": "",
  "signature": "sha256:be44730ae90b66423a15a633f42442074efcf257603ecfb1ef523581ee679065"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Learning the basics of Natural Language Toolkit (NLTK)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook introduces you to the basics of the Python Natural Language Toolkit (NLTK).\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Getting started. Before starting is good to install the nltk.data which contains many corpora, toy grammars, trained models, etc. \n",
      "Installation: at http://nltk.org/nltk_data/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From there you can choose Collections, Corpora, Models or All Packages "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "! Explain: What are 'tokens'?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sentence tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start with a short example of tokenizing a text in sentences. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = \"It was a great day - said John. But it was almost going to rain\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we want to split the text in sentences. First we need to import the sentence tokenizing function and after that we can put the text as an argument "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import sent_tokenize "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenize(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "['It was a great day - said John.', 'But it was almost going to rain']"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What about greek ? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "greek_text = \"\u03a0\u03c1\u03bf\u03ca\u03c3\u03c4\u03bf\u03c1\u03b9\u03ba\u03bf\u03af \u03c0\u03bf\u03bb\u03b9\u03c4\u03b9\u03c3\u03bc\u03bf\u03af \u03ac\u03c1\u03c7\u03b9\u03c3\u03b1\u03bd \u03bd\u03b1 \u03b1\u03bd\u03b1\u03c0\u03c4\u03cd\u03c3\u03c3\u03bf\u03bd\u03c4\u03b1\u03b9 \u03c3\u03c4\u03b1 \u0392\u03bf\u03c5\u03bb\u03b3\u03b1\u03c1\u03b9\u03ba\u03ac \u03b5\u03b4\u03ac\u03c6\u03b7 \u03ba\u03b1\u03c4\u03ac \u03c4\u03b7 \u039d\u03b5\u03bf\u03bb\u03b9\u03b8\u03b9\u03ba\u03ae \u03c0\u03b5\u03c1\u03af\u03bf\u03b4\u03bf. \\\n",
      "\u0397 \u03b1\u03c1\u03c7\u03b1\u03af\u03b1 \u03b9\u03c3\u03c4\u03bf\u03c1\u03af\u03b1 \u03c4\u03b7\u03c2 \u03b3\u03bd\u03ce\u03c1\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u03c0\u03b1\u03c1\u03bf\u03c5\u03c3\u03af\u03b1 \u03c4\u03c9\u03bd \u0398\u03c1\u03b1\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9 \u03b1\u03c1\u03b3\u03cc\u03c4\u03b5\u03c1\u03b1 \u03c4\u03c9\u03bd \u0395\u03bb\u03bb\u03ae\u03bd\u03c9\u03bd \u03ba\u03b1\u03b9 \u03c4\u03c9\u03bd \u03a1\u03c9\u03bc\u03b1\u03af\u03c9\u03bd.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = sent_tokenize(greek_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(k[0])\n",
      "print(k[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u03a0\u03c1\u03bf\u03ca\u03c3\u03c4\u03bf\u03c1\u03b9\u03ba\u03bf\u03af \u03c0\u03bf\u03bb\u03b9\u03c4\u03b9\u03c3\u03bc\u03bf\u03af \u03ac\u03c1\u03c7\u03b9\u03c3\u03b1\u03bd \u03bd\u03b1 \u03b1\u03bd\u03b1\u03c0\u03c4\u03cd\u03c3\u03c3\u03bf\u03bd\u03c4\u03b1\u03b9 \u03c3\u03c4\u03b1 \u0392\u03bf\u03c5\u03bb\u03b3\u03b1\u03c1\u03b9\u03ba\u03ac \u03b5\u03b4\u03ac\u03c6\u03b7 \u03ba\u03b1\u03c4\u03ac \u03c4\u03b7 \u039d\u03b5\u03bf\u03bb\u03b9\u03b8\u03b9\u03ba\u03ae \u03c0\u03b5\u03c1\u03af\u03bf\u03b4\u03bf.\n",
        "\u0397 \u03b1\u03c1\u03c7\u03b1\u03af\u03b1 \u03b9\u03c3\u03c4\u03bf\u03c1\u03af\u03b1 \u03c4\u03b7\u03c2 \u03b3\u03bd\u03ce\u03c1\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u03c0\u03b1\u03c1\u03bf\u03c5\u03c3\u03af\u03b1 \u03c4\u03c9\u03bd \u0398\u03c1\u03b1\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9 \u03b1\u03c1\u03b3\u03cc\u03c4\u03b5\u03c1\u03b1 \u03c4\u03c9\u03bd \u0395\u03bb\u03bb\u03ae\u03bd\u03c9\u03bd \u03ba\u03b1\u03b9 \u03c4\u03c9\u03bd \u03a1\u03c9\u03bc\u03b1\u03af\u03c9\u03bd.\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The sent_tokinze is working with punctianiational signs. It is good for the most of the european languages. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jp_text = \"\u30d6\u30eb\u30ac\u30ea\u30a2\u306e\u653f\u4f53\u306f\u5171\u548c\u5236\u3067\u3042\u308a\u3001\u56fd\u5bb6\u5143\u9996\u306f\u5927\u7d71\u9818\u3067\u3042\u308b\u3002\u5927\u7d71\u9818\u306f\u4e00\u671f5\u5e74\u306e\u4efb\u671f\u3067\u3001\u56fd\u6c11\u306e\u76f4\u63a5\u6295\u7968\u306b\u3088\u3063\u3066\u9078\u3070\u308c\u3001\\\n",
      "\u4e8c\u671f\u307e\u3067\u518d\u9078\u53ef\u80fd\u3067\u3042\u308b\u3002\u5927\u7d71\u9818\u306f\u56fd\u5bb6\u5143\u9996\u3067\u3042\u308b\u3068\u5171\u306b\u3001\u8ecd\u306e\u6700\u9ad8\u53f8\u4ee4\u5b98\u3001\u56fd\u5bb6\u5b89\u5168\u8aee\u554f\u4f1a\u8b70\u306e\u8b70\u9577\u3082\u517c\u306d\u308b\u3002\\\n",
      "\u5927\u7d71\u9818\u306f\u6cd5\u6848\u306e\u66f4\u306a\u308b\u5be9\u8b70\u306e\u70ba\u306b\u8b70\u4f1a\u3067\u53ef\u6c7a\u3055\u308c\u305f\u6cd5\u6848\u3067\u3082\u5dee\u623b\u3057\u3066\u518d\u5be9\u8b70\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u304c\u3001\\\n",
      "\u8b70\u4f1a\u306f\u591a\u6570\u306e\u8cdb\u6210\u306b\u3088\u308a\u5927\u7d71\u9818\u306e\u6cd5\u6848\u62d2\u5426\u3092\u8986\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenize(jp_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "['\u30d6\u30eb\u30ac\u30ea\u30a2\u306e\u653f\u4f53\u306f\u5171\u548c\u5236\u3067\u3042\u308a\u3001\u56fd\u5bb6\u5143\u9996\u306f\u5927\u7d71\u9818\u3067\u3042\u308b\u3002\u5927\u7d71\u9818\u306f\u4e00\u671f5\u5e74\u306e\u4efb\u671f\u3067\u3001\u56fd\u6c11\u306e\u76f4\u63a5\u6295\u7968\u306b\u3088\u3063\u3066\u9078\u3070\u308c\u3001\u4e8c\u671f\u307e\u3067\u518d\u9078\u53ef\u80fd\u3067\u3042\u308b\u3002\u5927\u7d71\u9818\u306f\u56fd\u5bb6\u5143\u9996\u3067\u3042\u308b\u3068\u5171\u306b\u3001\u8ecd\u306e\u6700\u9ad8\u53f8\u4ee4\u5b98\u3001\u56fd\u5bb6\u5b89\u5168\u8aee\u554f\u4f1a\u8b70\u306e\u8b70\u9577\u3082\u517c\u306d\u308b\u3002\u5927\u7d71\u9818\u306f\u6cd5\u6848\u306e\u66f4\u306a\u308b\u5be9\u8b70\u306e\u70ba\u306b\u8b70\u4f1a\u3067\u53ef\u6c7a\u3055\u308c\u305f\u6cd5\u6848\u3067\u3082\u5dee\u623b\u3057\u3066\u518d\u5be9\u8b70\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u304c\u3001\u8b70\u4f1a\u306f\u591a\u6570\u306e\u8cdb\u6210\u306b\u3088\u308a\u5927\u7d71\u9818\u306e\u6cd5\u6848\u62d2\u5426\u3092\u8986\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002']"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tokenizing words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Part-of-speech tagging "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Word frequencies "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Filtering stop words"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5. Cooccurrences (and ngrams)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
