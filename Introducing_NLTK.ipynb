{
 "metadata": {
  "name": "",
  "signature": "sha256:01af85f1fe88fad4070974bf996f4af34597b9308f56301e5057d003e2ed01ac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Learning the basics of Natural Language Toolkit (NLTK)\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Installing NLTK "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "This notebook has a few dependencies that can be installed via the python package manager -pip \n",
      "    \n",
      "    1. Python 3.0 or higher \n",
      "    2. NLTK \n",
      "    3. NLTK corpora\n",
      "\n",
      "If Python and pip are installed, you can install NLTK as follows:\n",
      "\n",
      "    ~$ pip install nltk \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Getting started. Before starting is good to install the nltk.data which contains many corpora, toy grammars, trained models, etc. \n",
      "Installation: at http://nltk.org/nltk_data/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This will open up a window with which you can download various corpora and models to a specified location. Take note of the download directory."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preprocessing Text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK is a good tool at the processing of a Raw text - it provides some tools for deviding text into it's constituent parts: \n",
      "\n",
      "- sent_tokenize: a punkt sentence tokenizer \n",
      "    \n",
      "    The sent_tokenize devides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations and words that start sentences. (Must be trained on a large collection of plaintext in the target language before it can be used) \n",
      "    \n",
      "    Punkt is designed to learn parameters (list of abbreviations, etc) unsupervised from a corpus similiar to the target domain. The pre-packaged models may therefore be unsuitable: -> PunktSentenceTokenizer(text) to learn parameters from the given text. \n",
      "    \n",
      "    \n",
      "- word_tokenize: a treebank tokenizer \n",
      "\n",
      "    The Treebank tokenizer that uses regular expressions to tokenize text as in PennTreebank. It assumes that the text has already been segmented into sentences, e.g. using first sent_tokenize() \n",
      "    \n",
      "    \n",
      "- pos_tag: a maximum entropy tagger trained on the Penn Treebank "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sentence tokenization \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is tokenization ? \n",
      "\n",
      "- Tokenization is the process of breaking of a text up into sentences, words, phrases or other meaningful elements called tokens \n",
      "\n",
      "- The tokanizaion is the pre-processing of the text analysis "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start with a short example of tokenizing a text in sentences. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = \"It was a great day - said John. But it was almost going to rain\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we want to split the text in sentences. First we need to import the sentence tokenizing function and after that we can put the text as an argument "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import sent_tokenize "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenize(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(sent_tokenize(text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What about greek ? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "greek_text = \"\u03a0\u03c1\u03bf\u03ca\u03c3\u03c4\u03bf\u03c1\u03b9\u03ba\u03bf\u03af \u03c0\u03bf\u03bb\u03b9\u03c4\u03b9\u03c3\u03bc\u03bf\u03af \u03ac\u03c1\u03c7\u03b9\u03c3\u03b1\u03bd \u03bd\u03b1 \u03b1\u03bd\u03b1\u03c0\u03c4\u03cd\u03c3\u03c3\u03bf\u03bd\u03c4\u03b1\u03b9 \u03c3\u03c4\u03b1 \u0392\u03bf\u03c5\u03bb\u03b3\u03b1\u03c1\u03b9\u03ba\u03ac \u03b5\u03b4\u03ac\u03c6\u03b7 \u03ba\u03b1\u03c4\u03ac \u03c4\u03b7 \u039d\u03b5\u03bf\u03bb\u03b9\u03b8\u03b9\u03ba\u03ae \u03c0\u03b5\u03c1\u03af\u03bf\u03b4\u03bf. \\\n",
      "\u0397 \u03b1\u03c1\u03c7\u03b1\u03af\u03b1 \u03b9\u03c3\u03c4\u03bf\u03c1\u03af\u03b1 \u03c4\u03b7\u03c2 \u03b3\u03bd\u03ce\u03c1\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u03c0\u03b1\u03c1\u03bf\u03c5\u03c3\u03af\u03b1 \u03c4\u03c9\u03bd \u0398\u03c1\u03b1\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9 \u03b1\u03c1\u03b3\u03cc\u03c4\u03b5\u03c1\u03b1 \u03c4\u03c9\u03bd \u0395\u03bb\u03bb\u03ae\u03bd\u03c9\u03bd \u03ba\u03b1\u03b9 \u03c4\u03c9\u03bd \u03a1\u03c9\u03bc\u03b1\u03af\u03c9\u03bd.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = sent_tokenize(greek_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(k[0])\n",
      "print(k[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(k)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The sent_tokinze is working with punctianiational signs. It is good for the most of the european languages. But let's try with spanish "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spanish_text = \"Existen cuatro maneras de responder preguntas. \u00bfCu\u00e1les son esas cuatro maneras?\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenize(spanish_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "In the nltk.data folder you can find under /tokenizer/punkt a various number of language tokenizer e.g. spanish.pickle "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spanish_tokenizer = nltk.data.load(\"tokenizers/punkt/spanish.pickle\")\n",
      "spanish_tokenizer.tokenize(spanish_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tokenizing sentences in words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_tokenize('Digital Hummanities')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = word_tokenize('Digital Hummanities') \n",
      "k[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "It works by separating words using spaces and punctuation. The function does not discard the punctuation. So after the word segmentation, we have to deal with the punctuation it. "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Another try"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_tokenize(\"can't go home\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In that example \"can't\" is separated, as \"ca\" and \"n't\".\n",
      "Alternative word tokenizers are the WordPunktTokenizer and PunktWordTokenizer. "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "WordPunktTokenizer splits all the punctuation into separate tokens"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import WordPunctTokenizer \n",
      "word_punct_tokenizer = WordPunctTokenizer()\n",
      "word_punct_tokenizer.tokenize(\"Can't jump today\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "PunktTokenizer splits on punctuation but keeps it within the word "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import PunktWordTokenizer \n",
      "punkt_word_tokenizer = PunktWordTokenizer()\n",
      "punkt_word_tokenizer.tokenize(\"Can't jump today\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Import the german.txt in the texts.file and try to tokenize the sentences and the words using te german.pickle  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Part-of-speech tagging "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Part-of-speech tagging is the process of converting a sentence in a list of words, grouping them into tuples (word,tag). \n",
      "The tag is a part-of-speech tag, which tells if the word is a noun, adjective, verb and so on. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = word_tokenize(\"It was a sunny and warm day, so we went to the beach\")\n",
      "nltk.pos_tag(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The pos_tag uses the PennTreebank."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = word_tokenize(\"Monty Python (sometimes known as The Pythons) were a British surreal comedy group who created \\\n",
      "their sketch comedy show Monty Python's Flying Circus,  th at first aired on the BBC on 5 October 1969.\")\n",
      "\n",
      "nltk.pos_tag(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Another interesting function is the text.similar function. It takes a word \"w\" and find in a given corpus all the pairs \"w1\"w\"\"w2\" \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
      " text.similar('greek')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Word frequencies "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK can also be used to do basic calculations on word corpora (= large amounts of text), such as counting words and creating frequency tables. Let's try it out with excerpts from Wikipedia's article on Monty Python (which is stored in the file texts/english.txt)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "tokens = word_tokenize(\"Monty Python (sometimes known as The Pythons) were a British surreal comedy group who created\\\n",
      "their sketch comedy show Monty Python's Flying Circus, that first aired on the BBC on 5 October 1969. Forty-five \\\n",
      "episodes were made over four series. The Python phenomenon developed from the television series into something larger \\\n",
      "in scope and impact, spawning touring stage shows, films, numerous albums, several books, and a stage musical. The \\\n",
      "group's influence on comedy has been compared to The Beatles' influence on music.\\\n",
      "Broadcast by the BBC between 1969 and 1974, Flying Circus was conceived, written and performed by its members Graham \\\n",
      "Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. Loosely structured as a sketch show, \\\n",
      "but with an innovative stream-of-consciousness approach (aided by Gilliam's animation), it pushed the boundaries of \\\n",
      "what was acceptable in style and content. A self-contained comedy team responsible for both writing and performing \\\n",
      "their work, the Pythons had creative control which allowed them to experiment with form and content, discarding rules\\\n",
      "of television comedy. Their influence on British comedy has been apparent for years, while in North America it has \\\n",
      "coloured the work of cult performers from the early editions of Saturday Night Live through to more recent absurdist\\\n",
      "trends in television comedy. 'Pythonesque' has entered the English lexicon as a result.\\\n",
      "In a 2005 UK poll to find The Comedian's Comedian, three of the six Pythons members were voted by fellow comedians\\\n",
      "and comedy insiders to be among the top 50 greatest comedians ever: Cleese at #2, Idle at #21, and Palin at #30.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For storing text, NLTK uses special text objects that are not just the simple string-objects you already know. Now let's create such a text object from the tokens."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.Text(tokens)\n",
      "# print out the type\n",
      "print(type(text))\n",
      "# print the object to check\n",
      "print(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can create a frequency distribution from this text object now, using NLTK's `FreqDist()` method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freqs = nltk.FreqDist(text)\n",
      "print(freqs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `FreqDist` object is similar to a dictionary containing keys and values. Keys represent words and values represent the counts.\n",
      "\n",
      "Let's now print the vocabulary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary = freqs.keys()\n",
      "print(vocabulary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The counts are stored as values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(freqs.values())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's now print them as a nice list next to each other."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for word in vocabulary:\n",
      "    print(word + \" \"+str(freqs[word]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see, several words occur multiple times (such as their/Their). We would like to make the text all lowercase for uniformity reasons."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lo_text = [word.lower() for word in text]\n",
      "freql = nltk.FreqDist(lo_text)\n",
      "print(freql)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is our new list:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for word in freql.keys():\n",
      "    print(word + \" \"+str(freql[word]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Find out how often the word 'python' occurs in the frequency list `freql`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#insert your code here\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Filtering stop words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"Stop words\" is the linguistic term for words that occur very often within a language, but have no semantic meaning on their own, such as 'the' or 'and'. Since they do not contribute to the 'substance' of a text, we sometimes need to filter them out prior to doing our calculations.\n",
      "\n",
      "NLTK has built-in lists of stop words for several languages. Let's just have look at the English stop word list:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "english_stopwords = stopwords.words(\"english\")\n",
      "print(english_stopwords)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Quiz!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Print out a list of turkish stop words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# insert your code here\n",
      "# ...\n",
      "\n",
      "print(turkish_stopwords)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's filter out the stop words from the Monty Python text and create a new frequency distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text3 = [word for word in lo_text if word not in english_stopwords]\n",
      "freq3 = nltk.FreqDist(text3)\n",
      "print(freq3.keys())\n",
      "print()\n",
      "for word in freq3.keys():\n",
      "    print(word + \" \"+str(freq3[word]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5. Cooccurrences"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cooccurrences (also called collocations) are sequences of words that occur together unusually often. NLTK has a built-in method for finding them.\n",
      "\n",
      "Let's try it out using our short Monty Python text:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "colloc = text.collocations()\n",
      "print(colloc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks valid. (Note: If applied to larger amounts of text, the results are usually a little bit more spectacular. ;) )\n",
      "\n",
      "You can try this out at home using your new NLTK knowledge. Have fun!\n",
      "\n",
      "\n",
      "*~ The end. ~*"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}